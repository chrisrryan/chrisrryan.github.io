[{
    "title": "About",
    "date": "",
    "description": "Chris Ryan",
    "body": "Developer - C# / Java / Javascript / Python / SQL\nTest Automation Engineer - Selenium / Cypress\nBSc(Hons) CompSci\nISTQB Certified\n",
    "ref": "/about/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  },{
    "title": "Rapid Software Testing (RST)",
    "date": "",
    "description": "RST is more a testing mindset and approach than methodology...",
    "body": "RAPID SOFTWARE TESTING (RST), authored by James Bach, is an advocate of exploratory testing to build an understanding of the product, its users and the risks. Other testing methodologies focus on creating test artifacts based upon features, in contrast with RST, it\u0026rsquo;s light on paperwork and rapid as it encourages focus on product stewardship and a de-emphasis on artifact collection and reporting counts\u0026hellip;\nWhich release meeting call do you want to be in on? Review Meeting One We ran a total of 2000 test case, 1900 passed and 100 failed. This graph shows the rate of test progress. We should think about pausing the release until we have these tests passing.\nReview Meeting Two We have one area of concern for this week\u0026rsquo;s release. On our user story for Customer Referral Discounts, maximum discount should continue to 15% with 5+ referrals but currently stops at 10%. This appears to be due to the time lag on the referral batch processing. Impact is increased support tickets, manual refunds and basket abandonment. However, only 1% of customers currently have 5+ referrals. The Defect Report has been emailed.\nClearly, the second scenario allow a useful discussion around impact, risk, mitigation and allows informed decision making. This in essence is what RST seeks to address; managers like metrics, but always have the story with the numbers and never numbers without the story.\nRST is a not prescriptive methodology, more a soft systems approach to software testing. It\u0026rsquo;s a holistic philosophy giving consideration to mission, product, relationships, user environment and deliverables. In terms of tooling, it largely relies on mind maps in preference to time consuming, traditional documentation.\nThis fits well with Agile, as a approach which embrace change, fast test cycles and clear communication. In some cases, such as health, military and safety-critical applications, this lean documentation would be unsatisfactory on it\u0026rsquo;s own. However, even here it could play a valuable supporting role, as a birds-eye view, a lightweight and easily absorbed communication tool.\nIn conclusion, RST provides many useful techniques in any tester toolkit.\nArticle by Chris Ryan\n",
    "ref": "/posts/rapid-software-testing/"
  },{
    "title": "Solving Wordle with Cypress",
    "date": "",
    "description": "Applying some automation to solving the daily Wordle problem...",
    "body": "Wordle Solver with Cypress Wordle was taking so much time! Applying some automation to solving the daily puzzle\u0026hellip; So, I\u0026rsquo;ve written some test automation to play it for me with Cypress, JavaScript and a list of 2,309 words. Check out my approach, code and video of it in action\u0026hellip;\nYes, the world is playing it so I\u0026rsquo;ve succumbed. I\u0026rsquo;ve been playing Wordle. It\u0026rsquo;s irresistible and you only get one word a day, all part of the magic.\nBut it\u0026rsquo;s also an irritating distraction, so the solution is automation. As Abraham Maslow said, \u0026lsquo;when all you have is a hammer, everything looks like a nail\u0026rsquo;. Well said, and it does indeed look like a nail.\nSo, I\u0026rsquo;ve fired up VS Code; together with some Javascript, Cypress and the list of 2,309 words Wordle uses for its answers. Here\u0026rsquo;s the result.\nThe Approach Cypress is a great tool with good documentation, and I wanted to experiment with it. I structured the code into two classes:\nPageActions class This handles the page interaction; button clicks, reads and records the response from Wordle. The shadow DOM on the Wordle page allows extraction of the score for each attempt: Green (correct), Yellow (present) and Grey (absent).\nSolver class The Solver class uses the word list used for its answers. It loads these into an array and as the responses come back it eliminates words from this array to narrow down to the correct solution. As starting point, it uses STARE as its first word, a reasonable first guess I feel. I\u0026rsquo;ve read some analysis that other words might be better, ADIEU for example but I\u0026rsquo;ll perhaps experiment with others later. Using the feedback from PageActions these word array is pruned; when a letter is correct then eliminate all words that letter(s) aren\u0026rsquo;t in that position, where is present remove all word that don\u0026rsquo;t contain it and that have that leter in that position and where a letter is absent then remove all words containing it. When making a guess if possible use a word with no repeat letters as that maximises our elimination or confirmation of letters.\nThis approach works well enough usually solving with four attempts which I feel is par for the puzzle. It could be improved using Information Theory as explained in the excellent Solving Wordle using information theory video but that\u0026rsquo;s for another day!\nThe Code Check my code here on GitHub.\nGitHub chrisrryan/wordle-cypress\nArticle by Chris Ryan\n",
    "ref": "/posts/wordle-with-cypress/"
  },{
    "title": "Solving Wordle with Selenium 4 & Java 17",
    "date": "",
    "description": "Applying some automation to solving the daily Wordle problem...",
    "body": "Wordle Solver with Selenium 4 As a comparison, based upon my Wordle automation solution using Cypress, this time I\u0026rsquo;ve used Selenium 4 and Java 17 to solve the puzzle.\nI\u0026rsquo;m not going to get into a full Selenium versus Cypress comparison here but I\u0026rsquo;d say both solutions work very well. Selenium has been around for many years but with version 4, the webdriver has been much improved with W3C compliance, integration with CDP (as Cypress uses) and relative locators for more stable scripts. Clearly, Cypress is gaining lots of traction which is understandable particularly with version 10 now available.\nI\u0026rsquo;ve used created two classes, a Solver class which deals with word selection and a PageActions class which handle UI interaction. Here\u0026rsquo;s it in action:\nCheck my code here on GitHub.\nGitHub chrisrryan/wordle-selenium-java\nArticle by Chris Ryan\n",
    "ref": "/posts/wordle-selenium/"
  },{
    "title": "Test Automation with .NET Core 6, RestSharp 1.07+ and NUnit",
    "date": "",
    "description": "Exploring .NET Core 6 with REST API Test Automation...",
    "body": "Test Automation with .NET Core 6, time to investigate .NET Core 6 and RESTSharp 1.07+ for Test Automation. NET CORE 6 is now stable and production ready, so I felt it was time to jump in and implement some test automation with it\u0026hellip;\nI\u0026rsquo;ve implemented a couple of basic smoke tests using https://api.zippopotam.us/ for some UK Postcode lookup and typicode.com for a POST test.\nI\u0026rsquo;m using RestSharp, which since version 1.07 had undergone a major upgrade, which contains quite a few breaking changes. I\u0026rsquo;m using 1.08 here and System.Text.Json.Serialization in place of Newtonsoft for some JSON framework speed improvement.\nI\u0026rsquo;ve added a couple of performance tests using System.Diagnostics.Stopwatch with a very generous test assertion that REST requests will complete in under 1000 milliseconds.\nThe Code Check my code here on GitHub.\nGitHub chrisrryan/autotest_dotnet6\nDemo Article by Chris Ryan\n",
    "ref": "/posts/dotnet6_test_automation/"
  },{
    "title": "The Test Automation Pyramid",
    "date": "",
    "description": "The Test Automation Pyramid concept that guides teams when it comes to prioritising the different kinds of testing in a test strategy.",
    "body": "WHAT IS THE TEST AUTOMATION PYRAMID? The Test Automation Pyramid, differentiates test types to maximise the benefits of each to deliver the optimal test strategy and return on investment. It\u0026rsquo;s an essential framework in the development of an effective test strategy.\nSelection Criteria to Maximise ROI The benefits of test automation may lead to a desire to automate everything. However, there\u0026rsquo;s a smarter way to approach this, to recognise the different types of tests and use each most effectively.\nClear-cut test objective Repeatable \u0026amp; adds value Meaningful Robust \u0026amp; Reliable With Agile and it\u0026rsquo;s accelerated product release cycles requires accelerated testing. With the focus on shorter release cycles, test automation can be hugely beneficial, if not essential.\nWith Continuous Integration/Continuous Deployment (CI/CD) methods, these benefits are amplified, automation allows test cases to be run each time code is pushed to the repository, ensuring bugs are found early and fixes made quickly. The process automates delivery of high quality code from development through to production.\nWhat Is the Test Automation Pyramid? The Test Automation Pyramid, guides teams in designing the optimal proportion of test approaches for the most effective automated test strategy.\nThe pyramid represents the three types of software tests:\nBase layer: Unit Tests Unit tests form the foundation, they test the smallest, most independent, they\u0026rsquo;re quick to write and run fast. These should be the largest element with your automated test suite and are the base of the pyramid.\nMiddle layer: Service Tests Service tests are the next layer of the pyramid. Service tests that bypass an application’s UI and talk directly to the API underneath it.\nService tests are usually slower and offer less precise feedback than unit tests. They might also be harder to write since they’d typically require access to the external dependencies. Despite their complexities, they have the benefit of painting a more realistic picture, by verifying the app’s behavior in a way that’s closer to how the user interacts with it than unit tests.\nThe Top of the Pyramid: UI Tests The top of the pyramid features what are called UI tests in Martin Fowler’s article, even though, in my definition, they’re more like end-to-end tests.\nThese are tests that drive the application the way a user would. These tests tend to be slow to run and very brittle since they might break due to the smallest change to the application’s UI. On the other hand, despite their problems, they do offer benefits. They mimic the way users interact with the application with much more realism than the other kinds of tests. Their results offer feedback that’s less precise. But at the same time, they might catch defects that the other types of testing would not.\nTo balance this, the testing pyramid says you should leverage this type of testing. Just make sure to have as few of them as you can get away with.\nConsider the Test Automation Pyramid for Getting the Best Out of Your Testing Approach Software testing is an essential activity. And what’s more, leveraging test automation is essential for tech organizations that want to deliver code as fast as possible while keeping quality high.\nHowever, resources aren’t infinite. You have to use some strategy to decide how to allocate your resources when working on testing. The test automation pyramid is a metaphor that guides you on how to make those decisions.\nArticle by Chris Ryan\n",
    "ref": "/posts/test-automation-pyramid/"
  },{
    "title": "The Test Engineering Dictionary",
    "date": "",
    "description": "Some key test engineering terms:",
    "body": "The Test Engineering Dictionary Teams need to share a vocabulary; so everyone has clarity. Without it, misunderstanding occurs. It\u0026rsquo;s vital to effective communication. Here\u0026rsquo;s a list of some key terms:\nKey Terms: A acceptance criteria: The exit criteria that a component or system must satisfy in order to be accepted by a user, customer, or other authorized entity. [IEEE 610]\nacceptance testing: Formal testing with respect to user needs, requirements, and business processes conducted to determine whether or not a system satisfies the acceptance criteria and to enable the user, customers or other authorized entity to determine whether or not to accept the system. [After IEEE 610]\naccessibility testing: Testing to determine the ease by which users with disabilities can use a component or system. [Gerrard]\naccuracy: The capability of the software product to provide the right or agreed results or effects with the needed degree of precision. [ISO 9126] See also functionality testing.\nad hoc testing: Testing carried out informally; no formal test preparation takes place, no recognized test design technique is used, there are no expectations for results and randomness guides the test execution activity.\nagile testing: Testing practice for a project using agile methodologies, such as extreme programming (XP), treating development as the customer of testing and emphasizing the test-first design paradigm.\nalpha testing: Simulated or actual operational testing by potential users/customers or an independent test team at the developers’ site, but outside the development organization. Alpha testing is often employed as a form of internal acceptance testing.\nanomaly: Any condition that deviates from expectation based on requirements specifications, design documents, user documents, standards, etc. or from someone’s perception or experience. Anomalies may be found during, but not limited to, reviewing, testing, analysis, compilation, or use of software products or applicable documentation. [IEEE 1044] See also defect, deviation, error, fault, failure, incident, problem.\navailability: The degree to which a component or system is operational and accessible when required for use. Often expressed as a percentage. [IEEE 610]\nB back-to-back testing: Testing in which two or more variants of a component or system are executed with the same inputs, the outputs compared, and analyzed in cases of discrepancies. [IEEE 610]\nbehavior: The response of a component or system to a set of input values and preconditions.\nbenchmark test: (1) A standard against which measurements or comparisons can be made. (2) A test that is be used to compare components or systems to each other or to a standard as in (1). [After IEEE 610]\nbeta testing: Operational testing by potential and/or existing users/customers at an external site not otherwise involved with the developers, to determine whether or not a component or system satisfies the user/customer needs and fits within the business processes. Beta testing is often employed as a form of external acceptance testing in order to acquired feedback from the market.\nblack box testing: Testing, either functional or non-functional, without reference to the internal structure of the component or system.\nboundary value test: A black box test design technique in which test cases are designed based on boundary values.\nbranch testing: A white box test design technique in which test cases are designed to execute branches.\nbusiness process-based testing: An approach to testing in which test cases are designed based on descriptions and/or knowledge of business processes.\nC Capability Maturity Model (CMM): A five level staged framework that describes the key elements of an effective software process. The Capability Maturity Model covers practices for planning, engineering and managing software development and maintenance. [CMM]\ncapture/playback tool: A type of test execution tool where inputs are recorded during manual testing in order to generate automated test scripts that can be executed later (i.e. replayed). These tools are often used to support automated regression testing.\nclassification tree method: A black box test design technique in which test cases, described by means of a classification tree, are designed to execute combinations of representatives of input and/or output domains. [Grochtmann]\ncode coverage: An analysis method that determines which parts of the software have been executed (covered) by the test suite and which parts have not been executed, e.g. statement coverage, decision coverage or condition coverage.\ncomponent integration testing: Testing performed to expose defects in the interfaces and interaction between integrated components.\ncomponent specification: A description of a component’s function in terms of its output values for specified input values under specified conditions, and required non-functional behavior (e.g. resource-utilization).\ncomponent testing: The testing of individual software components. [After IEEE 610]\nconcurrency testing: Testing to determine how the occurrence of two or more activities within the same interval of time, achieved either by interleaving the activities or by simultaneous execution, is handled by the component or system. [After IEEE 610]\ncondition testing: A white box test design technique in which test cases are designed to execute condition outcomes.\nconfiguration management: A discipline applying technical and administrative direction and surveillance to: identify and document the functional and physical characteristics of a configuration item, control changes to those characteristics, record and report change processing and implementation status, and verify compliance with specified requirements. [IEEE 610]\ncoverage: The degree, expressed as a percentage, to which a specified coverage item has been exercised by a test suite.\nD data driven testing: A scripting technique that stores test input and expected results in a table or spreadsheet, so that a single control script can execute all of the tests in the table. Data driven testing is often used to support the application of test execution tools such as capture/playback tools. [Fewster and Graham] See also keyword driven testing.\ndecision condition testing: A white box test design technique in which test cases are designed to execute condition outcomes and decision outcomes.\ndecision table: A table showing combinations of inputs and/or stimuli (causes) with their associated outputs and/or actions (effects), which can be used to design test cases.\ndefect: A flaw in a component or system that can cause the component or system to fail to perform its required function, e.g. an incorrect statement or data definition. A defect, if encountered during execution, may cause a failure of the component or system.\nE equivalence partition: A portion of an input or output domain for which the behavior of a component or system is assumed to be the same, based on the specification.\nerror: A human action that produces an incorrect result. [After IEEE 610]\nerror seeding: The process of intentionally adding known defects to those already in the component or system for the purpose of monitoring the rate of detection and removal, and estimating the number of remaining defects. [IEEE 610]\nexercised: A program element is said to be exercised by a test case when the input value causes the execution of that element, such as a statement, decision, or other structural element.\nexhaustive testing: A test approach in which the test suite comprises all combinations of input values and preconditions.\nexploratory testing: Testing where the tester actively controls the design of the tests as those tests are performed and uses information gained while testing to design new and better tests. [Bach]\nF fault tolerance: The capability of the software product to maintain a specified level of performance in cases of software faults (defects) or of infringement of its specified interface. [ISO 9126] See also reliability.\nfault tree analysis: A method used to analyze the causes of faults (defects).\nfeature: An attribute of a component or system specified or implied by requirements documentation (for example reliability, usability or design constraints). [After IEEE 1008]\nfunctional requirement: A requirement that specifies a function that a component or system must perform. [IEEE 610]\nfunctional testing: Testing based on an analysis of the specification of the functionality of a component or system. See also black box testing.\nG glass box testing: See white box testing.\nH heuristic evaluation: A static usability test technique to determine the compliance of a user interface with recognized usability principles (the so-called “heuristics”).\nI impact analysis: The assessment of change to the layers of development documentation, test documentation and components, in order to implement a given change to specified requirements.\nincremental development model: A development life cycle where a project is broken into a series of increments, each of which delivers a portion of the functionality in the overal project requirements. The requirements are prioritized and delivered in priority order in the appropriate increment. In some (but not all) versions of this life cycle model, each subproject follows a ‘mini V-model’ with its own design, coding and testing phases.\nincremental testing: Testing where components or systems are integrated and tested one or some at a time, until all the components or systems are integrated and tested.\nincident management: The process of recognizing, investigating, taking action and disposing of incidents. It involves recording incidents, classifying them and identifying the impact. [After IEEE 1044]\nintegration testing: Testing performed to expose defects in the interfaces and in the interactions between integrated components or systems. See also component integration testing, system integration testing.\ninterface testing: An integration test type that is concerned with testing the interfaces between components or systems.\nL load test: A test type concerned with measuring the behavior of a component or system with increasing load, e.g. number of parallel users and/or numbers of transactions to determine what load can be handled by the component or system.\nM maintenance: Modification of a software product after delivery to correct defects, to improve performance or other attributes, or to adapt the product to a modified environment. [IEEE 1219]\nmaintenance testing: Testing the changes to an operational system or the impact of a changed environment to an operational system.\nmemory leak: A defect in a program’s dynamic store allocation logic that causes it to fail to reclaim memory after it has finished using it, eventually causing the program to fail due to lack of memory.\nmetric: A measurement scale and the method used for measurement. [ISO 14598]\nN negative testing: Tests aimed at showing that a component or system does not work. Negative testing is related to the testers’ attitude rather than a specific test approach or test design technique. [After Beizer].\nnon-functional testing: Testing the attributes of a component or system that do not relate to functionality, e.g. performance, security, reliability, efficiency, usability, maintainability and portability.\nO operational profile testing: Statistical testing using a model of system operations (short duration tasks) and their probability of typical use. [Musa]\noperational testing: Testing conducted to evaluate a component or system in its operational environment. [IEEE 610]\nP page object model (pom): A design pattern for user interface automation that aims to separate UI elements/HTML (which can change frequently) and the interaction with them, from the automation application components. The pattern provides encapsulation and abstraction which prevents code duplication and aids maintainability.\npass/fail criteria: Decision rules used to determine whether a test item (function) or feature has passed or failed a test. [IEEE 829]\nperformance testing: The process of testing to determine the performance of a software product. See efficiency testing.\nphase test plan: A test plan that typically addresses one test level.\npriority: The level of (business) importance assigned to an item, e.g. defect.\nQ quality: The degree to which a component, system or process meets specified requirements and/or user/customer needs and expectations. [After IEEE 610]\nquality assurance: Part of quality management focused on providing confidence that quality requirements will be fulfilled. [ISO 9000]\nquality management: Coordinated activities to direct and control an organization with regard to quality. Direction and control with regard to quality generally includes the establishment of the quality policy and quality objectives, quality planning, quality control, quality assurance and quality improvement. [ISO 9000]\nR random testing: A black box test design technique where test cases are selected, possibly using a pseudo-random generation algorithm, to match an operational profile. This technique can be used for testing non-functional attributes such as reliability and performance.\nregression testing: Testing of a previously tested program following modification to ensure that defects have not been introduced or uncovered in unchanged areas of the software, as a result of the changes made. It is performed when the software or its environment is changed.\nrequirement: A condition or capability needed by a user to solve a problem or achieve an objective that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed document. [After IEEE 610]\nrequirements-based testing: An approach to testing in which test cases are designed based on test objectives and test conditions derived from requirements, e.g. tests that exercise specific functions or probe non-functional attributes such as reliability or usability.\nrisk analysis: The process of assessing identified risks to estimate their impact and probability of occurrence (likelihood).\nrisk-based testing: Testing oriented towards exploring and providing information about product risks. [After Gerrard]\nrisk control: The process through which decisions are reached and protective measures are implemented for reducing risks to, or maintaining risks within, specified levels.\nrisk management: Systematic application of procedures and practices to the tasks of identifying, analyzing, prioritizing, and controlling risk.\nroot cause: An underlying factor that caused a non-conformance and possibly should be permanently eliminated through process improvement.\nS safety: The capability of the software product to achieve acceptable levels of risk of harm to people, business, software, property or the environment in a specified context of use. [ISO 9126]\nscalability testing: Testing to determine the scalability of the software product.\nsecurity: Attributes of software products that bear on its ability to prevent unauthorized access, whether accidental or deliberate, to programs and data. [ISO 9126]\nsecurity testing: Testing to determine the security of the software product.\nsmoke test: A subset of all defined/planned test cases that cover the main functionality of a component or system, to ascertaining that the most crucial functions of a program work, but not bothering with finer details. A daily build and smoke test is among industry best practices. See also intake test.\nstate transition testing: A black box test design technique in which test cases are designed to execute valid and invalid state transitions. See also N-switch testing.\nstatic analysis: Analysis of software artifacts, e.g. requirements or code, carried out without execution of these software artifacts.\nstress testing: Testing conducted to evaluate a system or component at or beyond the limits of its specified requirements. [IEEE 610]\nstub: A skeletal or special-purpose implementation of a software component, used to develop or test a component that calls or is otherwise dependent on it. It replaces a called component. [After IEEE 610]\nsystem integration testing: Testing the integration of systems and packages; testing interfaces to external organizations (e.g. Electronic Data Interchange, Internet).\nsystem testing: The process of testing an integrated system to verify that it meets specified requirements. [Hetzel]\nT technical review: A peer group discussion activity that focuses on achieving consensus on the technical approach to be taken. A technical review is also known as a peer review. [Gilb and Graham, IEEE 1028]\ntest automation: The use of software to perform or support test activities, e.g. test management, test design, test execution and results checking.\ntest case: A set of input values, execution preconditions, expected results and execution postconditions, developed for a particular objective or test condition, such as to exercise a particular program path or to verify compliance with a specific requirement. [After IEEE 610]\ntest charter: A statement of test objectives, and possibly test ideas. Test charters are amongst other used in exploratory testing. See also exploratory testing.\ntest condition: An item or event of a component or system that could be verified by one or more test cases, e.g. a function, transaction, quality attribute, or structural element.\ntest data: Data that exists (for example, in a database) before a test is executed, and that affects or is affected by the component or system under test.\ntest harness: A test environment comprised of stubs and drivers needed to conduct a test.\ntest infrastructure: The organizational artifacts needed to perform testing, consisting of test environments, test tools, office environment and procedures.\ntest level: A group of test activities that are organized and managed together. A test level is linked to the responsibilities in a project. Examples of test levels are component test, integration test, system test and acceptance test. [After TMap]\ntest log: A chronological record of relevant details about the execution of tests. [IEEE 829]\ntest performance indicator: A metric, in general high level, indicating to what extent a certain target value or criterion is met. Often related to test process improvement objectives, e.g. Defect Detection Percentage (DDP).\ntest phase: A distinct set of test activities collected into a manageable phase of a project, e.g. the execution activities of a test level. [After Gerrard]\ntest plan: A document describing the scope, approach, resources and schedule of intended test activities. It identifies amongst others test items, the features to be tested, the testing tasks, who will do each task, degree of tester independence, the test environment, the test design techniques and test measurement techniques to be used, and the rationale for their choice, and any risks requiring contingency planning. It is a record of the test planning process [After IEEE 829]\ntest process: The fundamental test process comprises planning, specification, execution, recording and checking for completion. [BS 7925/2]\ntest run: Execution of a test on a specific version of the test object.\ntest script: Commonly used to refer to a test procedure specification, especially an automated one.\ntest suite: A set of several test cases for a component or system under test, where the post condition of one test is often used as the precondition for the next one.\ntop-down testing: An incremental approach to integration testing where the component at the top of the component hierarchy is tested first, with lower level components being simulated by stubs. Tested components are then used to test lower level components. The process is repeated until the lowest level components have been tested.\nU usability testing: Testing to determine the extent to which the software product is understood, easy to learn, easy to operate and attractive to the users under specified conditions. [After ISO 9126]\nuse case testing: A black box test design technique in which test cases are designed to execute user scenarios.\nuser test: A test whereby real-life users are involved to evaluate the usability of a component or system.\nV V-model: A framework to describe the software development life cycle activities from requirements specification to maintenance. The V-model illustrates how testing activities can be integrated into each phase of the software development life cycle.\nvalidation: Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled. [ISO 9000]\nverification: Confirmation by examination and through the provision of objective evidence that specified requirements have been fulfilled. [ISO 9000]\nW walkthrough: A step-by-step presentation by the author of a document in order to gather information and to establish a common understanding of its content. [Freedman and Weinberg, IEEE 1028]\nwhite box testing: Testing based on an analysis of the internal structure of the component or system.\n",
    "ref": "/posts/dictionary/"
  }]
